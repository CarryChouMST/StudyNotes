- ![image-20230410190933944](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20230410190933944.png)

- 向量叉乘得出的结果是一个向量，其方向垂直于叉乘向量的平面，对于二维向量叉乘，可视为z向为0，则叉乘结果为0,0,z式的向量，然后可以通过z的正负判断点在直线的方位。

- A向量与B向量的叉乘AxB(A在前)，其方向为右手定则从A到B，大拇指的方向。

- 快速判断两个凸包面是否碰撞的方法，可以采用轴分离算法，具体细节如下

  ![image-20240328164602875](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240328164602875.png)

  ![image-20240328164520643](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240328164520643.png)

- 轴对齐BoundingBox和方向boundingBox![image-20240328165454412](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240328165454412.png)

  综上，分离轴定理是一种适用于 bounding box 和 polygon 的精细碰撞检测算法，其优点是算法原理简单，可准确判断两个多边形是否相交；缺点在于当多边形的边数较多时，该算法的效率较低（当两个多边形相交时，需要遍历完所有边进行判断）。

  在实际应用中，为了提高效率，通常**先使用 基于轴对齐包围矩形（AABB）的方法进行粗略的碰撞检测，然后再使用 分离轴定理（SAT）做精细碰撞检测**。

# 矩阵仿射变换

- 根据几何关系写出的旋转矩阵，其列向量恰好为旋转后坐标系（红色）的坐标轴在旋转前的坐标系（黑色）中的坐标，或者叫**方向余弦**。[image-20240902092757355](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240902092757355.png)

- 所谓**旋转自身左乘矩阵，旋转坐标系右乘矩阵**，描述的是一种旋转矩阵组合关系，**并不是在目标向量的左右侧**，而是指在前一个旋转矩阵的基础上，往左侧/右侧组合矩阵。

  ~~~cpp
  A = M1*B;
  A = M2*M1*B;左乘矩阵，旋转自身
  A = M1*M2*B;右乘矩阵，旋转坐标系。//意思就是从将坐标系从1到2之后的B向量，基于原始坐标系表示就类似于退栈，先左乘2退栈为基于1坐标系下的B向量，再左乘1表示基于原始坐标系下的B向量
  ~~~


- **坐标系**由**坐标原点**和**基向量**决定，**坐标原点**和**基向量**确定了，坐标系也就确定了。

  $$\left[\begin{matrix}x \\ y \end{matrix} \right]$$ = $$x\left[\begin{matrix}1 \\ 0 \end{matrix}\right] + y\left[\begin{matrix}0 \\ 1\end{matrix}\right] = \left[\begin{matrix}1 && 0 \\ 0&&1\end{matrix}\right]\left[\begin{matrix}x\\y\end{matrix}\right]$$

  其相对坐标原点在1， 0方向上的投影为x，在0，1方向上的投影为yy——这里投影的意思是过(x,y)做坐标轴的平行线与坐标轴的交点到原点的距离（数值乘以向量）

  旋转后，基向量分别变为（cosθ，sinθ）和（-sinθ，consθ）。

- 仿射矩阵就是基向量的变化和坐标原点的变化，第一列是x基向量的变化, 第二列是y，第三列是坐标原点

# 编码算法经验

- array与unordered_map，unordered_map基于键值对的删改查中有优势，尤其是查和删和改，**但是对于增，unordered_map需要做一次额外的hash，会耗费比较多的资源**

  ==同时如果数据对于遍历的需求比较高，修改也是批量修改而不是单点删，改，使用array会有较大的优势。因为unordered_map由于hash算法的映射会占用较多的空间，而且也带来遍历性能的损耗。==

- 相比于for循环，while循环比较动态，可以用于动态区间的循环/分段处理。比如如下：

  ~~~cpp
  std::priority_queue<int> Q;
  while (!Q.isEmpty())
  {
      auto cur = Q.pop();
      for (const auto it : cur)
      {
          ...
      }
  }  // 以上就实现了bfs的基本模板代码
  ~~~


- 并查集union-find需要有一个图边的数组以及根节点映射表，根节点映射表中的各个节点初始化为自己，然后基于该表定义join与find方法。 
- 堆是完全二叉树，可以使用数组实现，下标为0位置的不用。假设root为当前节点的下标，则其父节点为root/2；左子树为root\*2；右子树为root\*2+1
- dijikstra实际上是BFS+DP表，其中使用优先队列+visited数组进行排序优化

# 容器

## 区间信息/区间查询

### topk数集

- 全排序O(n(log(n)))

- 部分排序(O(nk))

- 只取部分不排序，维护小根堆/大根堆O(n(log(k)))

- 随机选择，减治法，O(n)

  快排伪代码

  ~~~cpp
  void q_sort(std::array<int> nums, int low, int high)
  {
      if (low >= high)
      {
          return;
      }
      
      int i = partition(arr, low, high);
      q_sort(nums, low, i-1);
      q_sort(nums, i + 1, high);
  }
  ~~~

- 【合并k个有序数组，每个长度为n】遇到影响参数超过三个的算法，首先考虑分治。基于等效树的思想，得到递归算法复杂度为$$O(nk\log(k))$$。

- 【海量数据中查重复出现的元素，BitMap，位图法】在海量数据中查找出重复出现的元素或者去除重复出现的元素也是常考的问题。针对此类问题，一般可以通过位图法BitMap实现。例如，已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。（==有点像计数排序，丢弃数据本身的特性，编码映射到一个固有的bit位。位图算法的实现可以使用一个char数组或者int数组，则位图增长的粒度为8或者32==）


## 数据结构

- 每个结点的度数等于其子结点的个数，即等于与它相连的下一层结点的个数

# 图论算法

- 有存图、最短路、连通分量计算、测环、最小生成树、拓扑排序、二分图匹配等经典问题。

## 存图

- 存图目前一般有三种方法：邻接矩阵（空间复杂度最高），邻接表（空间复杂度和时间复杂度中庸【因为有动态容器vector扩容损耗】）和链式前向星（前向星的升级版）【目前最优，通过使用**总边集数组**和**指针下标模拟**，将邻接链表完全静态化，避免频繁扩容】。

- 链式前向星包括一个头结点数组和边集数组

  ~~~cpp
  int head[25005]={0};
  edge e[50005*3];
  int index=1;
  void adde(int ai,int bi,int ci)
  {
  	e[index].to=bi;
  	e[index].next=head[ai];
  	e[index].ci=ci;
  	head[ai]=index++;
  }
  
  int t,r,p,s;
  scanf("%d %d %d %d",&t,&r,&p,&s);
  for(int i=0;i<r;i++)
  {
      int ai,bi,ci;
      scanf("%d %d %d",&ai,&bi,&ci);
      adde(ai,bi,ci);
      adde(bi,ai,ci);
  
  ~~~


## 最短路

- [dijikstra算法](https://blog.csdn.net/michealoven/article/details/114040136)

  - dijikstra不能有负权边（负权环），本质上是没有考虑边可以被重复选取的情况。

- dijikstra算法与kruskal算法类似，都是拿到所有的有权边后基于贪心的思想处理，具体而言，初始化单源最短路数组（自己到自己为0），然后给一个松弛候选边列表，实时从松弛候选边列表中取出最近的边执行松弛行为，**松弛成功后继续加入队列【只有松弛成功的才加入队列，本质是遍历所有与原点直接/间接连接的边，只有松弛成功的边才有评估的价值，==同时，dijikstra认为，当前队列中边权最小的已经可以确定最短路【不可能有其他的边再对他进行松弛，但实际上有可能负值的间接边可以对其进行松弛，因此dijikstra不适用于有负权边的图。换句话说，Dijkstra算法在计算最短路径时，不会因为负边的出现而更新已经计算过(收录过)的顶点的路径长度|？感觉可以修复，只要把vis数组去掉就行】==**，考虑到多次逻辑取值与插入，使用优先队列存储侯选边。直到最后队列出栈完毕没有发现可松弛的边，即可得到单源最短路数组。

  ~~~cpp
  void dijikstra(int n, vector<vector<int>> mat)
  {
      int len = mat.size();
      vector a(len, 0x3f3f3f3f);
      a[n] = 0;
      priority_queue<pair<int, int>, vector<pair<int, int>>, greater<>> q;
      std::unordered_set<int> vis;  // 已经确定最短路的节点
      q.push(make_pair<0, n>);
      while (!q.empty())
      {
          auto pair = q.pop();
   		if (vis.count(pair.second))
          {
              continue;
          }
          vis.insert(pair.second);
          for (const auto i : len)
          {
              if((mat[pair.first][i] + pair.second) < a[i])
              {
                  a[i] = mat[pair.first][i] + pair.second;
                  
                  // push，只有松弛成功的边才有被评估的价值
                  q.push(a[i], i);
              }
          }
  }
  ~~~

- Dijkstra算法的时间复杂度为`O((V+E)logV)`，其中V表示图中的节点数，E表示图中的边数。具体来说，算法的时间复杂度主要取决于两个部分：节点的遍历和优先队列的操作。|==有疑问，如果按BFS+DP表理解，应该是O(ElogE)，在存在父权的情况下，每条边只会被访问一次。|已解决：两个不存在边的定点也会被视为有一条无限长的边，实际就是O(V+Elog(E))====

  在节点的遍历过程中，Dijkstra算法需要遍历所有的节点，这个过程的时间复杂度为O(V)。

  在优先队列的操作中，Dijkstra算法需要将节点按照距离值进行排序，并选择距离值最小的节点进行处理。在每次选择节点后，需要更新与该节点相邻的节点的距离值。这个过程中，每个节点最多会被处理一次，而每次处理时需要更新其相邻节点的距离值。由于优先队列的插入和删除操作的时间复杂度为O(logV)，因此总的时间复杂度为==O((V+E)logV)==。**【说是logV还是logE都可以，即使E=V2，也就是多了一倍】**

- 朴素dijikstra是V2，其差别在于使用数组（哈希表）作为评估集容器还是使用堆作为评估集容器，在实际使用中，即使是稠密图，堆优化的dijikstra复杂度也会小于普通版，因为E的常数项很小，不是所有的边都会被真的推入队列。

  | 稠密图       | 稀疏图           | 有负权边                            |            |
  | ------------ | ---------------- | ----------------------------------- | ---------- |
  | 单源问题     | Dijkstra+heap    | SPFA (或Dijkstra+heap,根据稀疏程度) | SPFA       |
  | APSP(无向图) | SPFA(优化)/Floyd | SPFA(优化)                          | SPFA(优化) |
  | APSP(有向图) | Floyd            | SPFA (或Dijkstra+heap,根据稀疏程度) | SPFA       |

- 【Floyd，弗洛伊德算法、插点法】多源最短路，三层循环，时间复杂度$O(n^3)$，空间复杂度$O(n^2)$。选取中介点k（一层循环），计算任意i（第二层），到j（第三层循环）的最短路是否可以被更新。可以用一个Path矩阵（==矩阵、多维图实际上就是一个正交表==），记录从i到j需要首先经过的点k1，在基于点k进行最短路径距离状态转移时，更新path矩阵。全部更新完毕后，递归查表可以得到路径。核心思想是动态规划。

- Floyd算法

  - 弗洛伊德属于动态规划的算法剖析【**动态规划刷表+滚动数组优化**】：
    - 状态：d\[k\]\[i\]\[j\]表示以0-k为空间节点时i到j的最短距离
    - 状态转移：d\[k+1\]\[i\]\[j\]中有可能有两种情况：最短路经过k+1/不经过k+1，则有$d[k+1][i][j] = min(d[k][i][j], d[k-1][i][k] + d[k-1][k][j]) (k,i,j \in [1, n])$
    - 滚动数组优化，复用$d[i][j]$，就变成了三层循环+松弛操作。

- SPFA(Shortest Path Faster Algorithm)——实际上是使用队列优化的Bellman-Ford算法，最坏还是VE，均摊kE（复杂度不稳定，）

  - 算法大致流程是用一个队列来进行维护。 初始时将源加入队列。 每次从队列中取出一个元素，并对所有与他相邻的点进行松弛，若某个相邻的点松弛成功，如果该点没有在队列中，则将其入队。 直到队列为空时算法结束。
  - 判断有无负环：如果某个点进入队列的次数超过V次则存在负环（SPFA无法处理带负环的图）
  - SPFA取消了贪心+剪枝，直接采用队列处理，基于输入顺序的不同复杂度不稳定，最坏VE，最好E，优点是可以判断负环。

- dijikstra相比于Bellman-Ford（或者SPFA）实际上**在引入不存在负权边假设后，做了贪心+剪枝**。

## 最小生成树

- 【Krustal算法】服务于最小生成树，就是从n条边中，找到组成最小连通分量的m-1（m-1 <= n）条边（贪心思想+并查集）

- 【并查集】并查集可以理解为一种数据结构，基于路径压缩和启发式合并后给定边可以以$O(\alpha(n))$的时间复杂度查询某节点的集合归属。

- 并查集时间复杂度|==反函数，就是对于值应用相同的函数，得到定义域。即未知量和值调换一下==

  同时使用路径压缩和启发式合并之后，并查集的每个操作平均时间仅为 $O(\alpha(n))$，其$\alpha(n)$ 为阿克曼函数的反函数，其增长极其缓慢，也就是说其单次操作的平均运行时间可以认为是一个很小的常数。

  [Ackermann 函数](https://en.wikipedia.org/wiki/Ackermann_function) ![A(m, n)](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) 的定义是这样的：

  ![image-20240621101908203](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240621101908203.png)

  而反 Ackermann 函数$\alpha(n)$的定义是阿克曼函数的反函数，即为最大的整数m使得$A(m, m) \leq n$ 。增长及其缓慢，一般不大于4。

另一种定义方法：**阿克曼反函数-----选定一个值下，结果大于该值的最小参数。**![image-20240621102920916](C:\Users\CARRYCHOU\AppData\Roaming\Typora\typora-user-images\image-20240621102920916.png)

- ==用于处理一些不相交集合（disjoint sets）的合并及查询问题。常常在使用中以**森林**来表示。==

- 并查集中的路径压缩是指在查找父节点的过程中，直接实时修改父节点映射表，将树的深度转变为其宽度，以压缩父节点查找树

  ~~~cpp
  // find+路径压缩
  int find(int x) {
      if (x != fa[x])  // x不是自身的父亲，即x不是该集合的代表
          fa[x] = find(fa[x]);  //查找x的祖先直到找到代表,于是顺手路径压缩
      return fa[x];
  }
  
  // find 没有路径压缩，单纯找
  int find(int x) {
  3   //寻找x的祖先
  4   if (fa[x] == x)  //如果x是祖先则返回
  5     return x;
  6   else
  7     return find(fa[x]);  //如果不是则x的爸爸问x的爷爷
  8 }
  ~~~

- 启发式合并/按秩合并，是指合并时的优化操作，这样可以保证树的高度得到控制

  ~~~cpp
  // 启发式合并
  int size[N];  //记录子树的大小
  void unionSet(int x, int y) {
    int xx = find(x), yy = find(y);
    if (xx == yy) return;
    if (size[xx] > size[yy])  //保证小的合到大的里
      swap(xx, yy);
    fa[xx] = yy;
    size[yy] += size[xx];
  }
  ~~~

- 因此，整个并查集的时间复杂度在使用路径压缩和启发式合并后，时间复杂度为**$O(m\alpha(n))$**，其中m为边数（操作数），n为节点数。

- 并查集除了可以用路径压缩法优化外，还可以用按秩合并法优化。按秩合并就是在对两个不同子集连接时，按照rank来连，也就是rank低的连在rank高的下面。rank高的做父亲节点，这样类似维护了一棵树，树是rank高的在上。因为路径压缩法优化程度更高，所以一般情况下使用路径压缩法。但是路径压缩法会破坏树的结构，在不想破坏树的结构的情况下，可以使用按秩合并法。

- 【Prim算法】
  - 基于点找生长边（kruskal是基于最小权重边找），有点像朴素dijikstra，找到n-1条最小生长边就行
- 【并查集】
  - 并查集的单次查询操作复杂度为阿克曼n，接近常数！

## 拓扑排序

- **时间复杂度**：O(n+m) 因每个结点只需加入队列一次，而每个结点加入队列前需要进行入度数量次的操作，因此时间复杂度为
  O(n+m)。【实际上就是初始化入队后，每个节点都要扫一遍自己的入度，==对于测环的情况，可以加一个计数器，队列出队一次计数，出循环后计数不等于点数则成环==】

  ~~~cpp
  bool bfs()
  {
      for(int i = 1; i <= n; i++)
      {
          if(inp[i] == 0) q.push(i);
      }
      while(!q.empty())
      {
          fir = q.front();
          q.pop();
          ans[++cnt] = fir;
          for(int i=0; i<v[fir].size(); i++)
          {
              sec = v[fir][i];
              inp[sec]--;
              if(inp[sec] == 0) q.push(sec);
          }
      }
      if(cnt == n) return 1;
      else return 0;
  }
  ~~~


## 二分图匹配

- 把一个图的顶点划分为两个不相交集U和V，使得每一条边都分别连接U、V中的顶点。如果存在这样的划分，则此图为一个二分图，一个等价定义是**不含有「含奇数条边的环」的图**。（实际上就是把所有的节点抑制某些边后层次化，基于森林层次奇偶性分组，组内不能有内连边。[示例图](.算法相关/image-20241212134921124.png)

- 图论中，一个「匹配」（matching）是一个边的集合，其中任意两条边都没有公共顶点。

- 图论中有匹配点、匹配边、未匹配点、未匹配边等概念。

  - 最大匹配：一个图所有匹配中，所含匹配边数最多的匹配，称为这个图的最大匹配。

  - 完美匹配：如果一个图的某个匹配中，所有的顶点都是匹配点，那么它就是一个完美匹配。[示例图4](.算法相关/image-20241212134921124.png)是一个完美匹配。显然，完美匹配一定是最大匹配。并非每个图都存在完美匹配。

  - **交替路**：从一个未匹配点出发，依次经过非匹配边、匹配边、非匹配边…形成的路径叫交替路。

  - **增广路**：从一个未匹配点出发，走交替路，如果途径另一个未匹配点（出发的点不算），则这条交替路称为增广路（agumenting path）。

  - 增广路的意义是**改进匹配**。只要把增广路中的匹配边和非匹配边的身份交换即可。由于中间的匹配节点不存在其他相连的匹配边，所以这样做不会破坏匹配的性质。交换后，图中的匹配边数目比原来多了 1 条。

  - **最大匹配数**：最大匹配的匹配边的数目

    **最小点覆盖数**：选取最少的点，使任意一条边至少有一个端点被选择

    **最大独立数**：选取最多的点，使任意所选两点均不相连

    **最小路径覆盖数**：对于一个 DAG（有向无环图），选取最少条路径，使得每个顶点属于且仅属于一条路径。路径长可以为 0（即单个点）。

### 二分图最大匹配

- 【匈牙利算法】匈牙利算法是一种在多项式时间内求解任务分配问题的组合优化算法，广泛应用在运筹学领域。**递归式冲突、协调。**

  ~~~cpp
  int N,M; // N:左侧元素个数 M:右侧元素个数
  int Map[MaxN][MaxM]; //邻接矩阵
  int mate[MaxM]; //右侧元素匹配的对象序号
  bool vis[MaxM]; //右侧元素在当前尝试分配的过程中是否被占用
   
  bool match(int i){ //为左侧第i个元素寻找对象
      for(int j=1;j<=M;++j){
          if(Map[i][j]&&!vis[j]){
              vis[j]=true;//想要把j号女生给i号男生
              if(mate[j]==0||match(mate[j])){//如果j号女生还没对象或者原对象还有别的选择，注意这里是指定新的左部节点递归匹配。
                  mate[j]=i;
                  return true;
              }
          }    
      }
      return false;
  }
   
  int Hungary(){
      int ans=0;
      for(int i=1;i<=N;++i){
          memset(vis,0,sizeof(vis)); //每一次尝试为一个新人让路的时候，总是优先考虑新人的匹配
          if(match(i))++ans;
      }   
      return ans;
  }
  ~~~

  

- 【增广路算法、匈牙利算法、Kuhn-Munkres算法】基于DFS找增广路，将二分图的边定向，变为基于dfs找有向图的一条指定规则的路径。时间复杂度O(VE)，遍历所有点，每个点遍历所有边找增广路。如果找到增广路，则将增广路上的所有边取反（非匹配变匹配、匹配变非匹配），**实现上可以通过递归+回溯实现。另外由于二分图是一种结构化的组织形式，在mch数组中可以只存右部图的节点、而在使用邻接表记录边时，只基于左部图记录。**


### 二分图最大权完美匹配

- 考虑到二分图中两个集合中的点并不总是相同，为了能应用 KM 算法解决二分图的最大权匹配，需要先作如下处理：将两个集合中点数比较少的补点，使得两边点数相同，再将不存在的边权重设为 ![0](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)，这种情况下，问题就转换成求 **最大权完美匹配问题**，从而能应用 KM 算法求解。
- KM算法是匈牙利算法的改造，将最大权的优化问题基于可行顶标+相等子图迭代的机制转化为最大(完美)匹配问题。
- 相等子图是原图的子集，可以理解为基于贪心的思想将权重更大的边逐个激活，直到找到完美匹配，即为最大权匹配。
-  KM算法一开始枚举n个点找增广路，为了找增广路需要延伸n次交错树，每次延伸需要n次维护，时间复杂度为O(n3)[最大权匹配](https://en.oi-wiki.org/graph/graph-matching/bigraph-weight-match/)。

# 动态规划

- 最优子结构与重叠子问题

# 图形是否封闭

- 判断图形是否封闭算法，可以就看图形的每个节点，至少要有一个出节点和入节点，无向图就看是否都挂着多于两条边，有向图还要多看下方向。

# 约束算法

- 还是以CAD设计的GCS为例，通常用**自由度，约束度，剩余自由度，和约束关系**内容来记录每个对象的状态(这里的自由度和刚体运动的自由度相同，而非有限元中的单元节点自由度)，用来记录图元的任意时刻的状态，并将状态用图来表示。
- 基于最大匹配的分解方法、基于构型识别的分解方法和基于结构刚性的分解方法
- 求解线性/非线性方程组，基于规则，迭代式求解方程组，同时为了高效仅支持单次迭代，并在迭代中采用DAG方式，将方程组化为多个不定方程求解，在解唯一的情况下，在DAG的末端往往方程组变为恰定。
- 【递归、层序遍历】一般能拆成层序处理尽量拆为层序处理，递归之后对于并发的支持不友好。
- 线性方程组包括共线，对齐（平行+点到直线的距离）等，非线性方程组包括角度约束（反三角函数）等

# 离散化算法

- 离散化（Discretization）指将连续特征或者变量，转变成离散特征或者变量的过程。
- 离散化的好处：
  - 1 分类算法中需要数据具有分类属性形式；
  - 2 可以抓住主要矛盾，克服数据中隐藏的缺陷；
  - 3 有利于对非线性关系的诊断和描述，离散化后变量每段的取值，如采取0，1的形式， 由一个变量派生为多个**哑变量**，最后分段描述自变量与因变量的关系。
  - **工程上说，离散后的数据相互独立，可以做并发/迭代处理。**
  - **one-hot变量编码和dummy变量（哑变量）编码**：
    - one-hot编码是基于事物的状态将其分为几类，比如猫(1, 0, 0)、狗(0, 1, 0)、兔子(0, 0, 1)。
    - 哑变量编码目前理解就是one-hot的降维版本，在确认一个类别全集后，使用补集的表示其中一个类别，比如猫(1, 0), 狗(0, 1)，兔子**（0， 0）**
- 离散化算法：
  - 基于熵的离散化：一般是离散化区间内的熵（信息量/不确定性）最小，区间之间的熵（信息量/不确定性）最大。注意**选用不同的区间/数据集作为度量的全集，每个样本出现的先验概率也就不一样，熵也不一样（熵最后还是衡量全集的纯度）**。
  - 比如(1, 2, 3, 4, 5)中，(1, 2, 3)有个熵A，(4, 5)有个熵B，(a, a, a, b, b)有个熵C。最优离散化函数f(x)为（A+B）/C的最小值。

# 统计学习算法

- 【熵，基尼系数】基尼系数与熵
  - 对于随机变量X，有$P(X = X_i) = P_i （i = 1, 2, ...）$。则定义其熵为$H(X) = -\sum\limits_{i=1}^{n}p_ilogp_i$。（之所以要加上负号，是因为logPi为负值）。
  - 对于同一个随机变量X，其基尼系数则被定义为$Gini(X) = \sum\limits_{n}^{i = 1}pi(1-pi) = 1- \sum\limits_{i=1}^{n}p_i^2$。【==基尼系数相当于熵模型(-logp)的一阶泰勒展开==】【基尼指数换个角度反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，指数越小，数据集纯度越高】
  - **信息熵和基尼系数都是被用来衡量信息的不确定性的工具。两种方法关键在于度量每一种可能性提供的不确定性的方法不一样，熵使用-log，基尼系数使用1-pi。**

